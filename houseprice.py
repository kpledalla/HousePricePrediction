# -*- coding: utf-8 -*-
"""HousePrice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Vam1upjZP6i5zO80_cfS2vz2eGBzJ7M
"""

# from google.colab import drive
# drive.mount('/content/drive')

# !pip install pyspark

from pyspark.sql import DataFrame, SparkSession

spark = SparkSession \
       .builder \
       .config("spark.driver.memory", "15g") \
       .appName("House Price Prediction") \
       .getOrCreate()

spark

# Commented out IPython magic to ensure Python compatibility.
from typing import List
import pyspark.sql.types as T
import pyspark.sql.functions as F
from pyspark import SparkFiles
from pyspark.sql.functions import isnull, when, count, col, desc

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore")

df_train = spark.read.csv("train.csv", header=True, inferSchema=True)
df_test = spark.read.csv("test.csv", header=True, inferSchema=True)


cols_type = ["BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "BsmtFullBath", "BsmtHalfBath", "GarageCars", "GarageArea"]
for c in cols_type:
  df_test = df_test.withColumn(c, df_test[c].cast('double'))

  # #changing even the columns in train set from int to int
  df_train = df_train.withColumn(c, df_train[c].cast('double'))

df_test.toPandas().info(verbose=False)

"""Removing duplicate rows if any"""

df_train.dropDuplicates()
df_test.dropDuplicates()

"""So no duplicate rows"""

df_train.show(5)


df_train.toPandas().describe(include = "object")


df_train = df_train.fillna(0) #We don't have any null values in train set, but doing
df_test = df_test.fillna(0)  # filling null values with 0 in test set

"""Features Alley, PoolQC, Fence, MiscFeature has more than 80% of missing values, so we can drop these columns"""

df_train = df_train.drop("FireplaceQu", "Fence", "Alley", "MiscFeature", "PoolQC")
df_test = df_test.drop("FireplaceQu", "Fence", "Alley", "MiscFeature", "PoolQC")

#LotFrontage, MasVnrArea, GarageYrBlt are numerical col with some NA, replace with 0 and cast to int

df_train = df_train.replace("NA", "0" , subset=['LotFrontage', 'MasVnrArea', 'GarageYrBlt', 'BsmtFullBath', 'BsmtHalfBath'])
df_test = df_test.replace("NA", "0" , subset=['LotFrontage', 'MasVnrArea', 'GarageYrBlt', 'BsmtFullBath', 'BsmtHalfBath'])

df_train = df_train.withColumn("LotFrontage",  df_train["LotFrontage"].cast('double')) \
                  .withColumn("MasVnrArea",  df_train["MasVnrArea"].cast('double')) \
                  .withColumn("BsmtFullBath",  df_train["BsmtFullBath"].cast('double')) \
                  .withColumn("BsmtHalfBath",  df_train["BsmtHalfBath"].cast('double')) \
                  .withColumn("GarageYrBlt",  df_train["GarageYrBlt"].cast('double'))
df_test = df_test.withColumn("LotFrontage",  df_test["LotFrontage"].cast('double')) \
                  .withColumn("MasVnrArea",  df_test["MasVnrArea"].cast('double')) \
                  .withColumn("BsmtFullBath",  df_test["BsmtFullBath"].cast('double')) \
                  .withColumn("BsmtHalfBath",  df_test["BsmtHalfBath"].cast('double')) \
                  .withColumn("GarageYrBlt",  df_test["GarageYrBlt"].cast('double'))


"""Features like LotArea, BsmtFinSF1, BsmtUnfSF, TotalBsmtSF, 2ndFlrSF, GrLivArea, MiscVal, WoodDeckSF and even the target feature SalePrice is having high deviation

In the remaining columns, replacing null values

In the categorical columns, NA represents feature not available. So replace these values with string "NONE" to avoid confusion
"""

column_types = df_train.dtypes
categoricalcolumns = [column for column, dtype in column_types if dtype == "string"]
continuousCols =  [column for column, dtype in column_types if ((dtype == "double" or dtype =='int') and str(column) != "SalePrice")]

df_train = df_train.replace('NA', 'NO', subset = categoricalcolumns)
df_test = df_test.replace('NA', 'NO', subset = categoricalcolumns)

mode_elec = df_train.groupBy(F.col('Electrical')).count().orderBy("count", ascending=False).first()[0]
df_train = df_train.replace('NA', mode_elec, subset = ['Electrical'])
df_test = df_test.replace('NA', mode_elec, subset = ['Electrical'])

df_train.toPandas().info(verbose=False)



#Features like LowQualFinSF,MiscVal,BsmtFinSF2,PoolArea,3SsnPorch,and ScreenPorch are having weak correlation with the target variable. So drop this features,

df_train = df_train.drop('LowQualFinSF','MiscVal', 'BsmtFinSF2', 'PoolArea', '3SsnPorch','ScreenPorch')
df_test = df_test.drop('LowQualFinSF','MiscVal', 'BsmtFinSF2', 'PoolArea', '3SsnPorch','ScreenPorch')

"""Creating a New Feature using all the columns storing "Bathroom Values."
FullBath: It shows total no. of Full bathrooms above grade.
HalfBath: It shows total no. of Half bathrooms above grade.
BsmtFullBath: It shows total no. of Basement full bathrooms.
BsmtHalfBath: It shows total no. of Basement half bathrooms.
"""

df_train = df_train.withColumn('Total_Bathrooms', df_train["FullBath"] + (0.5 * df_train["HalfBath"]) +
                               df_train["BsmtFullBath"] + (0.5 * df_train["BsmtHalfBath"]))
df_test = df_test.withColumn('Total_Bathrooms', df_test["FullBath"] + (0.5 * df_test["HalfBath"]) +
                               df_test["BsmtFullBath"] + (0.5 * df_test["BsmtHalfBath"]))
df_train = df_train.drop("FullBath", "HalfBath", "BsmtFullBath", "BsmtHalfBath")
df_test = df_test.drop("FullBath", "HalfBath", "BsmtFullBath", "BsmtHalfBath")

df_train = df_train.replace("Norm", "" , subset=['Condition2']) #Norm means normal which indicates there's no second condition

df_train = df_train.withColumn("Combined_Condition", F.concat(col("Condition1"), F.lit(" "), col("Condition2")))

df_train = df_train.drop("Condition1", "Condition2")

df_test = df_test.replace("Norm", "" , subset=['Condition2']) #Norm means normal which indicates there's no second condition

df_test = df_test.withColumn("Combined_Condition", F.concat(col("Condition1"), F.lit(" "), col("Condition2")))

df_test = df_test.drop("Condition1", "Condition2")

df_train = df_train.withColumn("Heating_Qu", F.concat(col("Heating"), F.lit(" - "), col("HeatingQC")))
df_train = df_train.drop("Heating", "HeatingQC")

df_test = df_test.withColumn("Heating_Qu", F.concat(col("Heating"), F.lit(" - "), col("HeatingQC")))
df_test = df_test.drop("Heating", "HeatingQC")

df_train.toPandas().info(verbose=False)

df_test.toPandas().info(verbose=False)

print(df_train.columns, "\n", df_test.columns)

from joblib import dump

from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import OneHotEncoder
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.regression import LinearRegression
#Applying string indexing to the categorical cols and removing the original cols

from pyspark.ml.feature import StringIndexer

column_types = df_train.dtypes
categoricalcolumns = [column for column, dtype in column_types if dtype == "string"]
continuousCols =  [column for column, dtype in column_types if (dtype != "string" and (str(column) != "SalePrice") and str(column) != "Id" )]

indexers = [ StringIndexer(inputCol=c, outputCol="{0}_ind".format(c))
              for c in categoricalcolumns ]

# default setting: dropLast=True
encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),
              outputCol="{0}_enc".format(indexer.getOutputCol()))
              for indexer in indexers ]

assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]
                            + continuousCols, outputCol="features")

pipeline = Pipeline(stages=indexers + encoders + [assembler])

preprocess_model = pipeline.fit(df_train)
assembledDF = preprocess_model.transform(df_train)
assembledDF_test = preprocess_model.transform(df_test)

# dump(preprocess_model, 'Inference_preprocess.joblib')

train = assembledDF.select("Id", "features", "SalePrice")
test =  assembledDF_test.select("Id", "features")

train = train.withColumn("SalePrice_final", F.log(col("SalePrice"))) # applying log transformation for saleprice

trainDF, testDF =  train.randomSplit([0.8,0.2], seed = 1234)

from pyspark.ml.regression import LinearRegression
# from pyspark.ml.linalg import SparseVector

lr = LinearRegression(featuresCol="features", labelCol="SalePrice_final", maxIter=10, regParam=0.3, elasticNetParam=0.8)

lrmodel = lr.fit(trainDF)
pred = lrmodel.transform(testDF)

# lrmodel.save('Inference_lrmodel')

pred.select("SalePrice",  F.exp(col("prediction"))).show()

# trainingSummary = lrmodel.summary
# print("numIterations: %d" % trainingSummary.totalIterations)
# print("objectiveHistory: %s" % str(trainingSummary.objectiveHistory))
# trainingSummary.residuals.show(5)
# print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
# print("r2: %f" % trainingSummary.r2)

from pyspark.ml.regression import RandomForestRegressor

rf = RandomForestRegressor(
    labelCol="SalePrice_final",
    featuresCol="features",
    numTrees=10,  # You can adjust the number of trees in the forest
    maxBins=10     # You can adjust the maximum number of bins for features
)

rf_model = rf.fit(trainDF)

rfpredictions = rf_model.transform(testDF)

from pyspark.ml.evaluation import RegressionEvaluator

evaluator = RegressionEvaluator(
    labelCol="SalePrice_final",
    predictionCol="prediction",
    metricName="rmse"  # You can use other metrics like "mse" or "r2" as well
)

# rmse = evaluator.evaluate(rfpredictions)
# print("Root Mean Squared Error (RMSE) for Random Forest Regressor= {:.2f}".format(rmse))

# rfpredictions.show(5)
# rf_model.save('Inference_rf_model')
# rfpredictions.select("SalePrice",  F.exp(col("prediction"))).show()

from pyspark.ml.regression import GBTRegressor

gbt = GBTRegressor(
    labelCol="SalePrice_final",
    featuresCol="features",
    maxIter=10,       # Number of boosting iterations (trees)
    maxDepth=5        # Maximum depth of each tree (default is 5)
)

gbt_model = gbt.fit(trainDF)
gbtpredictions = gbt_model.transform(testDF)
# gbtpredictions.show(5)

# rmse = evaluator.evaluate(gbtpredictions)
# print("Root Mean Squared Error (RMSE) for GBT Regressor = {:.2f}".format(rmse))

# gbt_model.save('Inference_gbt_model')
# gbtpredictions.select("SalePrice",  F.exp(col("prediction"))).show()

# pred_test = lrmodel.transform(test)
# pred_test.show(5)

pred_test = rf_model.transform(test)
# pred_test.show(5)

# pred_test = gbt_model.transform(test)
# pred_test.show(5)

pred_test.select("Id", F.exp(col("prediction")).alias("SalePrice")).show()